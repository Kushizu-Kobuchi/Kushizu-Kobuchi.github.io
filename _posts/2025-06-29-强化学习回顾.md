---
title: 强化学习回顾
date : 2025-06-29 18:52:54 +0800
categories: [机器学习, 强化学习]
tags: [计算机, 数学, 机器学习, 强化学习]
math: true
---

<!-- TOC -->

- [强化学习算法](#强化学习算法)
  - [Q-Learning](#q-learning)
  - [DQN](#dqn)
  - [PG](#pg)
  - [AC](#ac)
  - [TRPO](#trpo)
  - [PPO](#ppo)
- [Gym](#gym)
  - [Q-Learning](#q-learning-1)
  - [DQN](#dqn-1)
  - [PG](#pg-1)

<!-- /TOC -->

## 强化学习算法

### Q-Learning

最朴素的算法，维护一张Q表，Q(s,a)表示在状态s下采取动作a的期望回报，Q表更新公式为：

$$
Q(s,a) = Q(s,a) + \alpha(r + \gamma \max_{a'}Q(s',a') - Q(s,a))
$$

其中，$\alpha$是学习率，$\gamma$是折扣因子，取值为$0 \lt \gamma \lt 1$。，$Q(s',a')$表示在状态s'下采取动作a'的期望回报，$r$是当前状态s下采取动作a获得的即时回报。

Q-Learning使用$\epsilon$-greedy策略来选择动作，即以

$\epsilon$的概率随机选择动作，以$1-\epsilon$的概率选择当前状态下Q值最大的动作。

### DQN

Deep Q-Networks使用神经网络来拟合Q函数，将状态s和动作a作为神经网络的输入，输出为Q值。

DQN还使用了经验回放：将每个状态转移$(s,a,r,s')$存储在一个经验池中，每次更新Q值时，从经验池中随机采样一个状态转移，然后使用这个状态转移来更新Q值。因为神经网络训练期望样本是独立同分布的，但是强化学习采集的数据按顺序有很强的关联性。

DQN还使用了target network和evaluate network。target network用于计算下一状态的Q值，evaluate network用于计算策略选择的Q值，梯度下降发生在evaluate network上。target network的参数是固定的，每隔一段时间，将evaluate network的参数复制给target network。这样，target network的参数更新不会太频繁，可以减少训练过程中的不稳定性和波动。

观测状态转移：$(s_t, a_t, r_t, s_{t+1})$。
Q-Learning的目标：$Q(s,a) = Q(s,a) + \alpha(r + \gamma \max_{a'}Q(s',a') - Q(s,a))$
时间差分目标：$r + \gamma \max_{a'}Q(s',a') - Q(s,a)=0$
损失函数：$L(\theta) = f(r + \gamma \max_{a'}Q(s',a') - Q(s,a))$
梯度下降：$\theta = \theta - \alpha \nabla_\theta L(\theta)$

### PG

DQN的问题：无法表示随机策略，每次都使用最大值，输出选择不连续；无法表示连续动作。

策略梯度让神经网络直接输出策略函数$\pi(s,a;\theta)$。训练最大化累计回报$J(\theta) = V_\pi(s_0)$。

策略梯度定理：

$$\nabla J(\theta)=E_{s,a\sim\pi}[q_{\pi_\theta}\nabla_\theta\ln\pi_\theta(s,a)]$$

$q$是最终的收益的期望，应用蒙特卡洛，把这部分换成采样：

$$\nabla J(\theta)=E_{s,a\sim\pi}[G_t\nabla_\theta\ln\pi_\theta(s,a)]$$

$G_t$是从$(s,a)$开始的一条随机轨迹最后的回报。

### AC

时序差分方法是把$V(S_t)\leftarrow V(s_t)+\alpha[G_t-V(S_t)]$换成$(S_t)\leftarrow V(s_t)+\alpha[R_{t+1}+\gamma V(S_{t+1})-V(S_t)]$。视为估计$G_t$。

Actor-Critic算法中，Actor学习策略$\pi$得到尽可能高的回报，Critic学习估计策略的值函数$V$，评估Actor的表现。换句话说，Critic像Q-learing（学习评价当前状况），而Actor像PG（给出策略分布）。

最终的训练目标：
$$
L=\sum\log\pi_\theta(s_t,a_t)(r+\gamma V_(s_{t+1})-V(s_t))
$$

### TRPO

TRPO通过限制新策略和旧策略之间的差异来确保训练的稳定性。

优化目标：

$$
\max_\theta\mathbb{E}_{s,a\sim\pi_\theta}[\frac{\pi_\theta(a|s)}{\pi_{\theta_\text{old}}(a|s)}A^{\pi_{\theta_\text{old}}}(s,a)]
$$

$A$是优势函数，是在$s$处采取动作$a$的期望回报减去在$s$处的期望回报。

（理解为，找一个新策略，步长不大，比旧策略更好，）

每次步幅不大：

$$
\mathbb{E}_{s\sim\pi_\theta}[D_\text{KL}(\pi_{\theta_\text{old}}||\pi_\theta)]\leq\delta
$$

作泰勒展开，KL散度由二阶项

$$
D_KL(\theta'||\theta)\approx\frac{1}{2}(\theta'-\theta)^TH(\theta'-\theta)
$$

迭代
$$
\theta\leftarrow\argmax_{\theta'} g^T(\theta'-\theta)
$$

引入拉格朗日并求解：

$$
\theta\leftarrow\theta+\frac{1}{\lambda}H^{-1}g=\theta+\sqrt{\frac{2\lambda}{g^TH^{-1}g}}H^{-1}g
$$

### PPO

重要性采样



## Gym

一个gym类实现了
- `step()`：执行动作，返回下一个状态信息
- `reset()`：重置环境，返回初始状态信息
- `render()`，帮助可视化代理来渲染内容
- `close()`：关闭
- `action_space`：动作空间
- `observation_space`：状态空间
- `spec`：包含用于`make()`的初始化信息
- `metadata`：元数据
- `np_random`随机数生成器

以简单的倒摆小车来说

```py
env = gym.make('CartPole-v1')

env.observation_space
# Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)
# 水平位移 水平速度 转角 角速度

env.action_space
# Discrete(2)
# 向左和向右

env.reset()
# (array([-0.02547014, -0.02451906, -0.03074073, -0.01085381], dtype=float32),
#  {})
# observation: ObsType, info: Dict

env.step(env.step(env.action_space.sample()))
# (array([-0.02596052, -0.21918696, -0.03095781,  0.27197373], dtype=float32),
#  1.0,
#  False,
#  False,
#  {})
# observation: ObsType, reward: Float, terminated: Bool, truncated: bool, info: Dict
```

### Q-Learning

```py
class QLearningAgent:
    def __init__(self, state_dim: int, action_dim: int, learning_rate=0.5, reward_decay=0.99):
        """Q-Learning Agent
        Args:
            state_dim (int): 状态空间维度
            action_dim (int): 动作空间维度
            learning_rate (float, optional): 学习率. Defaults to 0.5.
            reward_decay (float, optional): 回报对时间的折扣. Defaults to 0.99.
        """
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.lr = learning_rate
        self.gamma = reward_decay
        self.q_table = np.random.uniform(0, 1, size=(state_dim, action_dim))
    
    
    def choose_action(self, state, epsilon):
        # epsilon表示探索的概率
        if np.random.uniform() < epsilon:
            action = np.random.randint(self.action_dim)
        else:
            action = np.argmax(self.q_table[state])
        return action

    def update(self, state, action, reward, state_next):
        self.q_table[state, action] += self.lr * (reward + self.gamma * np.max(self.q_table[state_next]) - self.q_table[state, action])
```

测试代码

```py
N_BINS = 6

env = gym.make('CartPole-v1')
action_space = env.action_space
state_space = env.observation_space
n_state = N_BINS ** 4
n_action = 2

q_learning_agent = QLearningAgent(n_state, n_action, learning_rate=0.5, reward_decay=1)

all_steps = [] # 统计坚持的时间
successive_success_episodes = 0 # 连续成功次数

n_episodes = 2000
n_steps_per_episode = 200

for i in tqdm(range(n_episodes)):
    observation, info = env.reset()
    done = False

    for j in range(n_steps_per_episode):
        
        state = digitize_state(observation)
        action = q_learning_agent.choose_action(state, 0.5 / (i+500))
        observation_next, reward, terminated, truncated, info = env.step(action)
        state_next = digitize_state(observation_next)
        done = terminated or truncated

        # 修改了回报，只有坚持了195步以上才给奖励
        if done:
            if j > 195:
                reward = 1
                successive_success_episodes += 1
            else:
                reward = -1
                successive_success_episodes = 0
        else:
            reward = 0
        
        q_learning_agent.update(state, action, reward, state_next)
        if done:
            break
        observation = observation_next
    
    all_steps.append(j)
    if successive_success_episodes >= 10:
        break
env.close()

np.save("./q_table.npy", q_learning_agent.q_table)

plt.plot(all_steps)
plt.xlabel("Episode")
plt.ylabel("steps_persevered")
plt.title("Q-Learning Training")
plt.grid()
plt.show()
```

### DQN

经验回放：

```py
class ReplayBuffer:
    def __init__(self, capacity):
        self.buffer = deque(maxlen=capacity)

    def push(self, state, action, next_state, reward):
        if next_state is None:
            next_state = np.zeros_like(state)
        self.buffer.append((state, action, next_state, reward))

    def sample(
        self, batch_size
    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        batch = random.sample(self.buffer, batch_size)
        (
            states,
            actions,
            next_states,
            rewards,
        ) = map(np.array, zip(*batch))
        return (
            torch.tensor(states),
            torch.tensor(actions, dtype=torch.int64).unsqueeze(1),
            torch.tensor(next_states),
            torch.tensor(rewards).unsqueeze(1),
        )

    def __len__(self):
        return len(self.buffer)
```

DQN
```py

class DQNAgent:
    def __init__(
        self,
        state_dim,
        action_dim,
        model_cls,
        learning_rate=0.0001,
        gamma=0.99,
        batch_size=32,
        replay_buffer_capacity=5000,
    ):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.model = model_cls(state_dim, action_dim)
        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)
        self.learning_rate = learning_rate
        self.loss_fn = F.smooth_l1_loss
        self.gamma = gamma
        self.batch_size = batch_size
        self.replay_buffer = ReplayBuffer(replay_buffer_capacity)

    def choose_action(self, state: torch.Tensor, epsilon) -> int:
        # epsilon表示探索的概率
        if np.random.uniform() < epsilon:
            action = random.randrange(self.action_dim)
        else:
            self.model.eval()
            with torch.no_grad():
                action = np.argmax(self.model(state))
        return int(action)

    def learn(self):

        if len(self.replay_buffer) < self.batch_size:
            return
        states, actions, next_states, rewards = self.replay_buffer.sample(self.batch_size)

        self.model.eval()
        q = self.model(states).gather(dim=1, index=actions)
        q1 = self.model(next_states).max(dim=1)[0].detach().unsqueeze(1)
        target = rewards + self.gamma * q1
        loss = self.loss_fn(q, target)

        self.model.train()
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
```

测试DQN：

```py
class SimpleMLP(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(SimpleMLP, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, action_dim),
        )

    def forward(self, x):
        return self.net(x)

env = gym.make("CartPole-v0")
state_space = env.observation_space
action_space = env.action_space
state_dim = state_space.shape[0]
action_dim = action_space.n

n_episodes = 500
n_steps_per_episode = 200
agent = DQNAgent(state_dim, action_dim, SimpleMLP)

all_steps = []  # 统计坚持的时间
successive_success_episodes = 0  # 连续成功次数


for i in tqdm(range(n_episodes)):
    observation, info = env.reset()

    for j in range(n_steps_per_episode):
        action = agent.choose_action(torch.Tensor(observation), 0.5 / (i + 1))
        observation_next, reward, terminated, truncated, info = env.step(action)
        done = terminated or truncated

        if done:
            next_state = None
            if j > 195:
                reward = 1
                successive_success_episodes += 1
            else:
                reward = -1
                successive_success_episodes = 0
        else:
            reward = 0

        agent.replay_buffer.push(observation, action, observation_next, reward)
        agent.learn()
        if done:
            break
        observation = observation_next
    all_steps.append(j)

    if successive_success_episodes >= 10:
        break

torch.save(agent.model.state_dict(), "cartpole_dqn.pth")

plt.plot(all_steps)

plt.xlabel("Episode")
plt.ylabel("steps_persevered")
plt.title("DQN Training")
plt.grid()
plt.show()
```

### PG

首先求$G_t = \sum _{i=t}^{T} \gamma^i r_i$

```py
def discounted_future_reward(rewards, gamma) -> list[float]:
    """计算未来奖励的折现值
    >>> discounted_future_reward([0.8, 0.8, 0.8, 0.8], 0.5)
    array([2.7512, 2.168 , 1.52  , 0.8   ])
    """
    discounted_future_reward = np.zeros_like(rewards)
    running_add = 0
    for t in reversed(range(0, len(rewards))):
        running_add = running_add * gamma + rewards[t]
        discounted_future_reward[t] = running_add
    return discounted_future_reward
```

PG模型需要输出概率：

```py
class SimpleMLPSoftmax(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(SimpleMLPSoftmax, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, action_dim),
            nn.Softmax()
        )
    
    def forward(self, x):
        return self.net(x)
```

训练

```py
class PGAgent:
    def __init__(
        self, 
        state_dim,
        action_dim,
        model_cls,
        learning_rate=3e-4,
        gamma=0.9):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.model = model_cls(state_dim, action_dim)
        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)
        self.gamma = gamma
    
    def choose_action(self, state: torch.Tensor) -> Tuple[int, torch.Tensor]:
        probs = self.model(state)
        choice = np.random.choice(self.action_dim, p=probs.detach().numpy())
        return choice, torch.log(probs[choice])

    def learn(self, rewards: list[float], log_probs: list[torch.Tensor]):
        discounted_rewards = torch.Tensor(discounted_future_reward(rewards, self.gamma))
        discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-9)
        
        policy_grads = - torch.stack(log_probs) * discounted_rewards
        
        self.optimizer.zero_grad()
        policy_grads = policy_grads.sum()
        policy_grads.backward()
        self.optimizer.step()
```

测试：

```py
env = gym.make("CartPole-v0")
state_space = env.observation_space
action_space = env.action_space
state_dim = state_space.shape[0]
action_dim = action_space.n

n_episodes = 2000
n_steps_per_episode = 200
agent = PGAgent(state_dim, action_dim, SimpleMLPSoftmax)

all_steps = []z

for i in tqdm(range(n_episodes)):
    observation, info = env.reset()
    rewards = []
    log_probs = []
    for j in range(n_steps_per_episode):
        action, log_prob = agent.choose_action(torch.Tensor(observation))
        observation_next, reward, terminated, truncated, info = env.step(action)
        rewards.append(reward)
        log_probs.append(log_prob)
        done = terminated or truncated
        
        if done:
            agent.learn(rewards, log_probs)
            all_steps.append(j)
            break
        observation = observation_next

torch.save(agent.model.state_dict(), "cartpole_pg.pth")

plt.plot(all_steps)

plt.xlabel("Episode")
plt.ylabel("steps_persevered")
plt.title("PG Training")
plt.grid()
plt.show()
```